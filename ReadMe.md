# An implementation of Matrix Factorization in Pytorch





## requirements

* pandas=1.0.3
* torch=1.5.0



## Results

```
loss: 34.88719
loss: 26.90128
loss: 20.86401
loss: 16.29381
loss: 12.79287
loss: 10.05780
loss: 7.87302
loss: 6.10237
loss: 4.68003
loss: 3.59373
loss: 2.84930
loss: 2.41908
loss: 2.21064
loss: 2.09490
loss: 1.97076
loss: 1.80454
loss: 1.61800
loss: 1.44952
loss: 1.32348
loss: 1.24206
loss: 1.19299
loss: 1.16033
loss: 1.13130
loss: 1.09843
loss: 1.05904
loss: 1.01382
loss: 0.96540
loss: 0.91717
loss: 0.87252
loss: 0.83411
loss: 0.80345
loss: 0.78056
loss: 0.76404
loss: 0.75145
loss: 0.74007
loss: 0.72771
loss: 0.71331
loss: 0.69707
loss: 0.68003
loss: 0.66357
loss: 0.64875
loss: 0.63612
loss: 0.62559
loss: 0.61668
loss: 0.60870
loss: 0.60101
loss: 0.59317
loss: 0.58495
loss: 0.57638
loss: 0.56767
loss: 0.55909
loss: 0.55092
loss: 0.54334
loss: 0.53645
loss: 0.53017
loss: 0.52438
loss: 0.51888
loss: 0.51354
loss: 0.50824
loss: 0.50299
loss: 0.49783
loss: 0.49283
loss: 0.48807
loss: 0.48359
loss: 0.47936
loss: 0.47535
loss: 0.47150
loss: 0.46773
loss: 0.46402
loss: 0.46035
loss: 0.45674
loss: 0.45320
loss: 0.44977
loss: 0.44647
loss: 0.44331
loss: 0.44027
loss: 0.43733
loss: 0.43449
loss: 0.43172
loss: 0.42903
loss: 0.42640
loss: 0.42385
loss: 0.42138
loss: 0.41899
loss: 0.41666
loss: 0.41440
loss: 0.41219
loss: 0.41003
loss: 0.40791
loss: 0.40583
loss: 0.40380
loss: 0.40181
loss: 0.39986
loss: 0.39797
loss: 0.39613
loss: 0.39433
loss: 0.39257
loss: 0.39086
loss: 0.38918
loss: 0.38754
loss: 0.38594
loss: 0.38437
loss: 0.38284
loss: 0.38134
loss: 0.37987
loss: 0.37843
loss: 0.37701
loss: 0.37562
loss: 0.37425
loss: 0.37291
loss: 0.37160
loss: 0.37031
loss: 0.36905
loss: 0.36781
loss: 0.36660
loss: 0.36541
loss: 0.36425
loss: 0.36311
loss: 0.36198
loss: 0.36088
loss: 0.35980
loss: 0.35874
loss: 0.35769
loss: 0.35667
loss: 0.35565
loss: 0.35466
loss: 0.35369
loss: 0.35273
loss: 0.35178
loss: 0.35086
loss: 0.34995
loss: 0.34905
loss: 0.34818
loss: 0.34731
loss: 0.34646
loss: 0.34563
loss: 0.34481
loss: 0.34400
loss: 0.34321
loss: 0.34242
loss: 0.34165
loss: 0.34089
loss: 0.34015
loss: 0.33941
loss: 0.33868
loss: 0.33797
loss: 0.33727
loss: 0.33658
loss: 0.33590
loss: 0.33523
loss: 0.33456
loss: 0.33391
loss: 0.33327
loss: 0.33264
loss: 0.33202
loss: 0.33140
loss: 0.33080
loss: 0.33020
loss: 0.32961
loss: 0.32903
loss: 0.32846
loss: 0.32789
loss: 0.32733
loss: 0.32678
loss: 0.32624
loss: 0.32571
loss: 0.32518
loss: 0.32466
loss: 0.32415
loss: 0.32364
loss: 0.32314
loss: 0.32265
loss: 0.32217
loss: 0.32169
loss: 0.32121
loss: 0.32074
loss: 0.32028
loss: 0.31983
loss: 0.31938
loss: 0.31893
loss: 0.31849
loss: 0.31806
loss: 0.31763
loss: 0.31721
loss: 0.31679
loss: 0.31638
loss: 0.31597
loss: 0.31557
loss: 0.31517
loss: 0.31477
loss: 0.31439
loss: 0.31400
loss: 0.31362
loss: 0.31324
loss: 0.31287
loss: 0.31251
loss: 0.31214
loss: 0.31178
loss: 0.31143
loss: 0.31108
loss: 0.31073
loss: 0.31039
loss: 0.31005
loss: 0.30971
loss: 0.30938
loss: 0.30905
loss: 0.30873
loss: 0.30841
loss: 0.30809
loss: 0.30777
loss: 0.30746
loss: 0.30715
loss: 0.30685
loss: 0.30655
loss: 0.30625
loss: 0.30595
loss: 0.30566
loss: 0.30537
loss: 0.30509
loss: 0.30480
loss: 0.30453
loss: 0.30425
loss: 0.30397
loss: 0.30370
loss: 0.30344
loss: 0.30317
loss: 0.30291
loss: 0.30265
loss: 0.30239
loss: 0.30214
loss: 0.30188
loss: 0.30164
loss: 0.30139
loss: 0.30115
loss: 0.30090
loss: 0.30066
loss: 0.30043
loss: 0.30019
loss: 0.29996
loss: 0.29973
loss: 0.29951
loss: 0.29928
loss: 0.29906
loss: 0.29884
loss: 0.29863
loss: 0.29841
loss: 0.29820
loss: 0.29799
loss: 0.29778
loss: 0.29757


Test MAE: 1.05559
```

